{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implemeting all necessery process to obtain the spectra that will be used in my Doctorate degreee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import zipfile\n",
    "import json \n",
    "import datetime\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import interpolate\n",
    "\n",
    "from astropy.time import Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WISeREP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main webpage is: [WISeREP](https://www.wiserep.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WISeREP                = \"www.wiserep.org\"\n",
    "url_wis_spectra_search = \"https://\" + WISeREP + \"/search/spectra\"\n",
    "\n",
    "# Specify the Personal api key here (*** MUST BE PROVIDED ***)\n",
    "personal_api_key       = \"9da72158cebc45da6305466dd99895b079219f6b\"\n",
    "# for User-Agent:\n",
    "WIS_USER_NAME          = \"Steve Jurado\"\n",
    "WIS_USER_ID            = \"Steve Jurado\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supernovae_types = {\n",
    "    1: 'SN', 2: 'SN I', 3: 'SN Ia', 4: 'SN Ib', 5: 'SN Ic', 6: 'SN Ib/c',\n",
    "    7: 'SN Ic-BL', 9: 'SN Ibn', 10: 'SN II', 11: 'SN IIP', 12: 'SN IIL', 13: 'SN IIn', 14: 'SN IIb', 15: 'SN I-faint', 16: 'SN I-rapid',\n",
    "    18: 'SLSN-I', 19: 'SNLS-II', 20: 'SNSN-R',\n",
    "    100: 'SN ia-pec', 102: 'SN ia-SC', 103: 'SN Ia-91bg-like', 104: 'SN Ia-91T-like', 105: 'SN iax[02cx-like]', 106: 'Sn Ia-CSM',\n",
    "    107: 'SN ib-pec', 108: 'SN Ic-pec', 109: 'SN Icn', 110: 'SN Ibn/Icn',\n",
    "    111: 'SN II-pec', 112: 'SN IIn-pec', 115: 'SN Ib-Ca-rich', 116: 'SN Ib7c-Ca-rich', 117: 'SN Ic-Ca-rich', 118: 'SN Ia-Ca-rich'\n",
    "}\n",
    "\n",
    "# spectypes = {10: 'Object', 50: 'Synthetic'}\n",
    "\n",
    "query_params    = \"&public=yes&type[]=\"+\"[\"+\",\".join(str(x) for x in supernovae_types.keys())+']'+\"&spectypes[]=10\"\n",
    "download_params_html = \"&num_page=250&format=html&files_type=ascii\"\n",
    "download_params_csv = \"&num_page=250&format=csv&files_type=ascii\"\n",
    "download_params = download_params_csv\n",
    "\n",
    "parameters_csv = \"?\" + query_params+download_params_csv + \"&personal_api_key=\" + personal_api_key\n",
    "parameters_html = \"?\" + query_params+download_params_html + \"&personal_api_key=\" + personal_api_key\n",
    "\n",
    "# url of wiserep spectra search (with parameters)\n",
    "URL      = url_wis_spectra_search + parameters_csv\n",
    "URL_html = url_wis_spectra_search + parameters_html\n",
    "\n",
    "print('The URL of WISeREP spectra search is:\\n',URL_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_string_json(string):\n",
    "    try:\n",
    "        json_object = json.loads(string)\n",
    "    except Exception:\n",
    "        return False\n",
    "    return json_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_status(response):\n",
    "    # external http errors\n",
    "    ext_http_errors       = [403, 500, 503]\n",
    "    err_msg               = [\"Forbidden\", \"Internal Server Error: Something is broken\", \"Service Unavailable\"]\n",
    "\n",
    "    json_string = is_string_json(response.text)\n",
    "    if json_string != False:\n",
    "        status = \"[ \" + str(json_string['id_code']) + \" - '\" + json_string['id_message'] + \"' ]\"\n",
    "    else:\n",
    "        status_code = response.status_code\n",
    "    if status_code == 200:\n",
    "        status_msg = 'OK'\n",
    "    elif status_code in ext_http_errors:\n",
    "        status_msg = err_msg[ext_http_errors.index(status_code)]\n",
    "    else:\n",
    "        status_msg = 'Undocumented error'\n",
    "    status = \"[ \" + str(status_code) + \" - '\" + status_msg + \"' ]\"\n",
    "    return status\n",
    "\n",
    "def print_response(response, page_num):\n",
    "    status = response_status(response)\n",
    "    stats = 'Page number ' + str(page_num) + ' | return code: ' + status        \n",
    "    print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------\n",
    "# current date and time\n",
    "current_datetime = datetime.datetime.now()\n",
    "current_date_time = current_datetime.strftime(\"%Y%m%d\")\n",
    "\n",
    "# current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# current download folder\n",
    "current_download_folder = os.path.join(cwd, \"wiserep_spectra_data\")\n",
    "os.makedirs(current_download_folder, exist_ok=True)\n",
    "\n",
    "# marker and headers\n",
    "wis_marker = 'wis_marker{\"wis_id\": \"' + str(WIS_USER_ID) + '\", \"type\": \"user\", \"name\": \"' + WIS_USER_NAME + '\"}'\n",
    "headers = {'User-Agent': wis_marker}\n",
    "\n",
    "# check file extension\n",
    "if \"format=tsv\" in download_params:\n",
    "    extension = \".tsv\"\n",
    "elif \"format=csv\" in download_params:\n",
    "    extension = \".csv\"\n",
    "elif \"format=json\" in download_params:\n",
    "    extension = \".json\"\n",
    "else:\n",
    "    extension = \".txt\"\n",
    "\n",
    "# meta data list and file\n",
    "META_DATA_LIST = []\n",
    "META_DATA_FILE = os.path.join(cwd, \"wisrep_spectra_metadata\" + extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page number\n",
    "page_num = 0\n",
    "page_num_max = 185\n",
    "\n",
    "# go trough every page\n",
    "while page_num < page_num_max:\n",
    "\n",
    "    # url for download\n",
    "    url = URL + \"&page=\" + str(page_num)\n",
    "    \n",
    "    # send requests\n",
    "    response = requests.post(url, headers = headers, stream = True)\n",
    "    \n",
    "    # chek if response status code is not 200\n",
    "    if (response.status_code != 200):\n",
    "        # if there are no more pages for download, don't print response, \n",
    "        # only print if response is something else\n",
    "        if response.status_code != 404:\n",
    "            print_response(response, page_num + 1)\n",
    "            page_num += 1\n",
    "        continue\n",
    "    \n",
    "    # print response\n",
    "    print_response(response, page_num + 1)\n",
    "    \n",
    "    # download data\n",
    "    file_name = 'wiserep_spectra.zip'\n",
    "    file_path = os.path.join(current_download_folder, file_name)\n",
    "    with open(file_path, 'wb') as f:\n",
    "        for data in response:\n",
    "            f.write(data)\n",
    "    \n",
    "    # unzip data\n",
    "    zip_ref = zipfile.ZipFile(file_path, 'r')\n",
    "    zip_ref.extractall(current_download_folder)\n",
    "    zip_ref.close()\n",
    "    # remove .zip file\n",
    "    os.remove(file_path)            \n",
    "    \n",
    "    # take meta data file\n",
    "    downloaded_files = os.listdir(current_download_folder)\n",
    "    meta_data_file = os.path.join(current_download_folder, [e for e in downloaded_files if 'wiserep_spectra' in e][0])          \n",
    "    # read meta data file\n",
    "    f = open(meta_data_file,'r')\n",
    "    meta_data_list = f.read().splitlines()\n",
    "    f.close()\n",
    "    # write this meta data list to the final meta data list\n",
    "    if page_num == 0:\n",
    "        META_DATA_LIST = META_DATA_LIST + meta_data_list\n",
    "    else:\n",
    "        META_DATA_LIST = META_DATA_LIST + meta_data_list[1:]         \n",
    "    # increase page number \n",
    "    page_num = page_num + 1                 \n",
    "    # remove meta data file\n",
    "    os.remove(meta_data_file)\n",
    "\n",
    "# write meta data list to file         \n",
    "if META_DATA_LIST != []:\n",
    "    f = open(META_DATA_FILE, 'w')\n",
    "    for i in range(len(META_DATA_LIST)):\n",
    "        if i == len(META_DATA_LIST) - 1:\n",
    "            f.write(META_DATA_LIST[i])\n",
    "        else:\n",
    "            f.write(META_DATA_LIST[i] + '\\n')\n",
    "    f.close()\n",
    "    print()\n",
    "    print (\"Wiserep data was successfully downloaded.\")\n",
    "    #print (\"Folder /wiserep_data_\" + current_date_time + \"/ containing the data was created.\")\n",
    "    print (\"Folder /wiserep_data_\" + \"/ containing the data was created.\")\n",
    "    #print (\"File spectra_\" + current_date_time + extension + \" was created.\")\n",
    "    print (\"File spectra_\" + extension + \" was created.\")\n",
    "else:\n",
    "    print (\"There is no WISeREP data for the given parameters.\")\n",
    "    shutil.rmtree(current_download_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fidx = 0\n",
    "file_name = os.listdir(path='wiserep_data_')[fidx]\n",
    "\n",
    "columns = ['lambda', 'flux_lambda']\n",
    "\n",
    "data = pd.read_csv(filepath_or_buffer='wiserep_data_/' + file_name, delim_whitespace=True)\n",
    "data.columns = columns\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(x='lambda',y='flux_lambda', kind='line')\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('flux')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the total amount of spectral obtained from WISeREP is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The total amount of spectra:',len(os.listdir(path='./wiserep_spectra_data')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the directory when the spectral processed will be save\n",
    "os.makedirs(name='data/wiserep_spectra',exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To select one file to prove some \n",
    "fidx = 1\n",
    "file_name = os.listdir(path='wiserep_spectra_data')[fidx]\n",
    "file_name.split(sep='_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_input = \"./wiserep_data_/\"\n",
    "file = 'SN2007af_2007-04-10_09-30-14_Lick-3m_KAST_UCB-SNDB.flm'\n",
    "file_split = file.split(sep='_')[3:]\n",
    "with open(PATH_input+file, 'r') as infile:\n",
    "    content = infile.readlines()\n",
    "    for line in content:\n",
    "        new_line = line.strip() + \"\\t\"+\"\".join(file_split)\n",
    "        print(new_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_INPUT  = './wiserep_spectra_data/'\n",
    "PATH_OUTPUT = './data/wiserep_spectra/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_types_files(path_input: str) -> dict:\n",
    "\n",
    "    \"\"\"\n",
    "    Return a dictionary with all supernova names of the spectra files\n",
    "    based on the file's type.\n",
    "    \n",
    "    Parameters: \n",
    "    -----------\n",
    "    path_input -> src: folder where there are the spectra.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    SN_Dict -> dict: Dictionary with all names of SN\n",
    "    \"\"\"\n",
    "\n",
    "    all_files = [files for files in os.listdir(path=path_input)]\n",
    "\n",
    "    # Select just those begin with SNLS\n",
    "    SNLS_list_files = [file for file in all_files if file.startswith('SNLS')]     \n",
    "    SNLS_list_names = [snname.split('_')[0] for snname in SNLS_list_files]\n",
    "    SNLS_list_names = list(set(SNLS_list_names))\n",
    "    \n",
    "    # Select whose that begin with other prefix\n",
    "    other_SN_list_files = [file for file in all_files if (file.startswith('2MASS') or not file.startswith('SN'))]\n",
    "    other_SN_list_files = [file for file in other_SN_list_files if not file[1].isdigit()]\n",
    "    other_SN_names = [snname.split('_')[0] for snname in other_SN_list_files]\n",
    "    other_SN_names = list(set(other_SN_names))\n",
    "\n",
    "    # Select SN that begins with SN but not continue with the year\n",
    "    no_SNSNLS_list_files = [file for file in all_files if file.startswith('SN')] \n",
    "    no_SNSNLS_list_files = [file for file in no_SNSNLS_list_files if not file.startswith('SNLS')] \n",
    "    no_SNSNLS_list_files = [file for file in no_SNSNLS_list_files if not file[2].isdigit()]\n",
    "    no_SNSNLS_list_names = [snname.split('_')[0] for snname in no_SNSNLS_list_files]\n",
    "    no_SNSNLS_list_names = list(set(no_SNSNLS_list_names))\n",
    "    no_SNSNLS_list_names\n",
    "    \n",
    "    # Select SN that begins with SN and continue with the year.\n",
    "    # This is the mosst complicated part.\n",
    "    SN_list_files = [file for file in all_files if file.startswith('SN')] \n",
    "    SN_list_files = [file for file in SN_list_files if file not in SNLS_list_files ]         \n",
    "    SN_list_files = [file for file in SN_list_files if file not in no_SNSNLS_list_files]         \n",
    "    SN_list_names = [snname.split('_')[0] for snname in SN_list_files]\n",
    "    SN_list_names = [snnameidx.replace('SN', '') for snnameidx in SN_list_names]\n",
    "    SN_list_names = list(set(SN_list_names))\n",
    "\n",
    "    SN_rest_list_files = [file for file in all_files if file not in SNLS_list_files]\n",
    "    SN_rest_list_files = [file for file in SN_rest_list_files if file not in no_SNSNLS_list_files]\n",
    "    SN_rest_list_files = [file for file in SN_rest_list_files if file not in other_SN_list_files]\n",
    "    SN_rest_list_files = [file for file in SN_rest_list_files if file not in SN_list_files]\n",
    "    \n",
    "    # Weird files\n",
    "    weird_SN_list_files = [file for file in SN_rest_list_files if not file[3].isdigit()]\n",
    "    weird_SN_list_names = ['_'.join(snname.split('_')[:2]) for snname in weird_SN_list_files]\n",
    "    weird_SN_list_names = list(set(weird_SN_list_names))\n",
    "\n",
    "    # Joining the big amount of SN\n",
    "    SN_rest_list_files = [file for file in SN_rest_list_files if file not in weird_SN_list_files]\n",
    "    SN_rest_names = [snname.split('_')[0] for snname in SN_rest_list_files]\n",
    "    SN_rest_names = [snname for snname in SN_rest_names if snname not in SN_list_names]\n",
    "    SN_rest_names = list(set(SN_rest_names))\n",
    "    \n",
    "    SN_names = SN_list_names | SN_rest_names\n",
    "    \n",
    "    # Weird files\n",
    "\n",
    "    SN_dict = {'SN': SN_names,\n",
    "               'SNLS': SNLS_list_names,\n",
    "               'Other_SN': no_SNSNLS_list_names,\n",
    "               'Other':other_SN_names,\n",
    "               'Weird':weird_SN_list_names}\n",
    "\n",
    "    return SN_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SN_dict = dictionary_types_files(path_input=PATH_INPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def files2raw(sn_name: str, path_input:str, path_output:str, key_value: bool =None) -> None:\n",
    "    \n",
    "    \"\"\"Join all the spectra files in one\n",
    "\n",
    "    Parameters:\n",
    "    ------------\n",
    "\n",
    "    Returns:\n",
    "    ------------\n",
    "    sn_name_raw.dat -> text file with the columns to be transformed in a dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if key_value == None:\n",
    "        return print('Error you need a key_value !!')\n",
    "\n",
    "    if key_value == 'Weird':\n",
    "        return print('This is a weird file')\n",
    "        \n",
    "    sn_pattern = re.compile(sn_name, re.IGNORECASE)\n",
    "    matching_files = []\n",
    "    # Buscar archivos que coincidan con el patrón\n",
    "    for file in os.listdir(path=path_input):\n",
    "        if sn_pattern.search(file):\n",
    "            matching_files.append(file)\n",
    "\n",
    "    # Crear el archivo de salida utilizando el nombre base del patrón\n",
    "    if key_value == 'SN':\n",
    "\n",
    "        with open(path_output + f'SN{sn_name}_raw.dat', 'w') as f:\n",
    "            for file in matching_files:\n",
    "                instrument = file.split(sep='_')[3:]\n",
    "                hour = file.split(sep='_')[2].replace('-',':')\n",
    "                date = file.split(sep='_')[1]\n",
    "                #print(instrument)\n",
    "                with open(path_input + file, 'r') as infile:\n",
    "                    content = infile.readlines()\n",
    "                    for line in content:\n",
    "                        if line.strip() and line.strip()[0].isdigit():\n",
    "                            new_line = line.strip() + \"\\t\" + \"_\".join(instrument) + \"\\t\" + \"T\".join([date,hour])\n",
    "                            f.write(new_line)\n",
    "                            f.write('\\n')\n",
    "                infile.close()\n",
    "        f.close()\n",
    "\n",
    "    else:\n",
    "        with open(path_output + f'{sn_name}_raw.dat', 'w') as f:\n",
    "            for file in matching_files:\n",
    "                instrument = file.split(sep='_')[3:]\n",
    "                hour = file.split(sep='_')[2].replace('-',':')\n",
    "                date = file.split(sep='_')[1]\n",
    "                with open(path_input + file, 'r') as infile:\n",
    "                    content = infile.readlines()\n",
    "                    for line in content:\n",
    "                        if line.strip() and line.strip()[0].isdigit():\n",
    "                            new_line = line.strip() + \"\\t\" + \"_\".join(instrument) + \"\\t\" + \"T\".join([date,hour])\n",
    "                            f.write(new_line)\n",
    "                            f.write('\\n')\n",
    "                infile.close()\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_raw_list = [file for file in os.listdir(path='./data/spectra/') if '_raw' in file]\n",
    "file_data_list = [file for file in os.listdir(path='./data/spectra/') if '_raw' not in file] \n",
    "file_raw_list_name = [file_name.split('_')[0] for file_name in file_raw_list]\n",
    "\n",
    "len(file_raw_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file2dataframe(sn_name:str, path_input:str, path_output:str, key_value:bool=None) -> None:\n",
    "    \n",
    "    \"\"\"Takes the raw.dat file and it corvert to 'dataframe'\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if key_value == None:\n",
    "        return print('Error u need a key_value !!')\n",
    "        \n",
    "    if key_value == 'Weird':\n",
    "        return print('This is a weird file')\n",
    "\n",
    "    data = []\n",
    "\n",
    "    if key_value == 'SN':\n",
    "        # Leer el archivo línea por línea\n",
    "        with open(f\"{path_output}SN{sn_name}_raw.dat\", 'r') as file:\n",
    "            for line in file:\n",
    "                # Dividir la línea en columnas\n",
    "                columns = re.split(r'\\s+', line.strip())\n",
    "                # Si la fila tiene menos de 4 columnas, agregar NaN y permutar\n",
    "                if len(columns) == 4:\n",
    "                    columns.append(np.nan)  # Añadir NaN\n",
    "                    columns[2], columns[-1] = columns[-1], columns[2]  # Permutar la tercera con la cuarta columna\n",
    "                    columns[3], columns[-1] = columns[-1], columns[3]  # Permutar la tercera con la cuarta columna\n",
    "                # Si la fila tiene más de 4 columnas, la truncamos\n",
    "                if len(columns) > 4:\n",
    "                    columns = columns[:6]\n",
    "                data.append(columns)\n",
    "\n",
    "        # Crear un DataFrame con los datos procesados\n",
    "        headers = ['lambda', 'flux_lambda', 'e_flux_lambda', 'instrument', 'date']\n",
    "        df = pd.DataFrame(data, columns=headers)\n",
    "        # Asegurarse de que la columna 'date' sea de tipo string\n",
    "        df['date'] = df['date'].astype(str)\n",
    "        # Convertir cada fecha a MJD\n",
    "        df['mjd'] = df['date'].apply(lambda x: Time(x).mjd)\n",
    "\n",
    "        # Eliminar la columna date\n",
    "        df = df.drop('date', axis=1)\n",
    "        df = df.reindex(columns=['mjd','lambda', 'flux_lambda', 'e_flux_lambda', 'instrument'])\n",
    "        # Guardar el DataFrame resultante en un nuevo archivo\n",
    "        df.to_csv(f\"{path_output}SN{sn_name}.dat\", index=False, header=True, na_rep='nan')\n",
    "        os.remove(f\"{path_output}SN{sn_name}_raw.dat\")\n",
    "\n",
    "    else: \n",
    "        with open(f\"{path_output}{sn_name}_raw.dat\", 'r') as file:\n",
    "            for line in file:\n",
    "                # Dividir la línea en columnas\n",
    "                columns = re.split(r'\\s+', line.strip())\n",
    "                # Si la fila tiene menos de 4 columnas, agregar NaN y permutar\n",
    "                if len(columns) == 4:\n",
    "                    columns.append(np.nan)  # Añadir NaN\n",
    "                    columns[2], columns[-1] = columns[-1], columns[2]  # Permutar la tercera con la cuarta columna\n",
    "                    columns[3], columns[-1] = columns[-1], columns[3]  # Permutar la tercera con la cuarta columna\n",
    "                # Si la fila tiene más de 4 columnas, la truncamos\n",
    "                if len(columns) > 4:\n",
    "                    columns = columns[:6]\n",
    "                data.append(columns)\n",
    "\n",
    "        # Crear un DataFrame con los datos procesados\n",
    "        headers = ['lambda', 'flux_lambda', 'e_flux_lambda', 'instrument', 'date']\n",
    "        df = pd.DataFrame(data, columns=headers)\n",
    "        # Asegurarse de que la columna 'date' sea de tipo string\n",
    "        df['date'] = df['date'].astype(str)\n",
    "        # Convertir cada fecha a MJD\n",
    "        df['mjd'] = df['date'].apply(lambda x: Time(x).mjd)\n",
    "\n",
    "        # Eliminar la columna date\n",
    "        df = df.drop('date', axis=1)\n",
    "        df = df.reindex(columns=['mjd','lambda', 'flux_lambda', 'e_flux_lambda', 'instrument'])\n",
    "        # Guardar el DataFrame resultante en un nuevo archivo\n",
    "        df.to_csv(f\"{path_output}{sn_name}.dat\", index=False, header=True, na_rep='nan')\n",
    "        os.remove(f\"{path_output}{sn_name}_raw.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_merge_spectra(sn_name: str, path_input: str, path_output: str, key_value: bool =None):\n",
    "    \n",
    "    if sn_name.startswith('SN') and key_value == 'SN':\n",
    "        sn_name = sn_name.replace('SN', '')\n",
    "    #print(sn_name)\n",
    "    if key_value == None:\n",
    "        return print('Error u need a key_value !!')\n",
    "        \n",
    "    if key_value == 'Weird':\n",
    "        return print('This is a weird file')\n",
    "    \n",
    "    if key_value == 'SN':\n",
    "        if f'SN{sn_name}.dat' in os.listdir(path=path_output):\n",
    "           return print('File already exist')\n",
    "        \n",
    "        #else:\n",
    "            #print('Running SN',sn_name)\n",
    "        try:\n",
    "            #print('Obtaining the raw data ...')\n",
    "            files2raw(sn_name = sn_name,path_input=path_input, path_output = path_output,key_value = key_value)\n",
    "        except:\n",
    "            print('Supernova SN',sn_name,'| Error in raw')\n",
    "        try:\n",
    "            #print('Obtaining the dataframe ....')\n",
    "            file2dataframe(sn_name=sn_name, path_input=path_output, path_output=path_output,key_value=key_value)\n",
    "        except:\n",
    "            print('Supernova SN',sn_name,'| Error in dataframe')\n",
    "\n",
    "    else:\n",
    "        if f'{sn_name}.dat' in os.listdir(path=path_output):\n",
    "            return print('File already exist')\n",
    "        #else:\n",
    "            #print('Running SN',sn_name)\n",
    "        #files2raw(sn_name=sn_name,path_input=path_input, path_output=path_output,key_value=key_value)\n",
    "        try:\n",
    "            #print('Obtaining the raw data ...')\n",
    "            files2raw(sn_name=sn_name,path_input=path_input, path_output=path_output,key_value=key_value)\n",
    "        except:\n",
    "            print('Supernova ',sn_name,'| Error in raw')\n",
    "        try:\n",
    "            #print('Obtaining the dataframe ....')\n",
    "            file2dataframe(sn_name=sn_name, path_input=path_output, path_output=path_output,key_value=key_value)\n",
    "        except:\n",
    "            print('Supernova ',sn_name,'| Error in dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SN_dict_keys = list(SN_dict.keys())\n",
    "SN_dict_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SN_dict['Other_SN'].remove('SN')\n",
    "SN_dict['Other_SN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ya estuvo SN, 0\n",
    "# Ya estuvo SNLS, 1\n",
    "# Ya estuvo Other_SN, 2\n",
    "# Ya estuvo Other, 3\n",
    "# No he usado Weird, 4\n",
    "\n",
    "key_value = SN_dict_keys[2]\n",
    "count = 0\n",
    "total = len(SN_dict[key_value])\n",
    "for sn_name in SN_dict[key_value]:\n",
    "    if count % 10 == 0:\n",
    "        print(f'{count}/{total}')\n",
    "    running_merge_spectra(sn_name=sn_name, path_input=PATH_INPUT, path_output=PATH_OUTPUT,key_value=str(key_value))\n",
    "    count += 1\n",
    "\n",
    "print('\\nEnd...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = os.listdir(path='./data/spectra/')\n",
    "all_files = [file for file in all_files if \"_raw\" not in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_name = random.choice(all_files)\n",
    "sn_name = 'SN2017awk.dat'\n",
    "data = pd.read_csv('./data/spectra/'+sn_name)\n",
    "\n",
    "def plotobject(df,sn_name, inst_name):\n",
    "  fig, ax = plt.subplots(figsize=(10, 6))\n",
    "  for mjd_date, group_df in df.groupby('mjd'):\n",
    "       ax.plot(group_df['lambda'], group_df['flux_lambda'], label=f'{mjd_date:.2f} MJD')\n",
    "  ax.legend(frameon=False)\n",
    "  ax.set_xlabel('Lambda')\n",
    "  ax.set_ylabel('Flux_lambda')\n",
    "  ax.set_title('Supernova:'+sn_name.split(\".\")[0]+\" | Instrument: \"+inst_name.split(\".\")[0])\n",
    "  \n",
    "\n",
    "data_instrument = data.groupby('instrument')\n",
    "for inst_name, group in data_instrument:\n",
    "    plotobject(group, sn_name=sn_name, inst_name=inst_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotobject(df):\n",
    "    df.groupby([\"date\"]).apply(\n",
    "        lambda df:ax.plot(df['lambda'], df['flux'], alpha=0.3))  \n",
    "fig, ax = plt.subplots(figsize=(24, 6))\n",
    "data.groupby('instrument').apply(plotobject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Master DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_INPUT = './data/spectra_wiserep/'\n",
    "\n",
    "all_files = [file for file in os.listdir(path=PATH_INPUT) if '_raw' not in file]\n",
    "print(f'The total amount of Supernovae ready to process is:',len(all_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supernovae_metadata = pd.read_csv('./wiserep_spectra_metadata.csv',low_memory=False)\n",
    "supernovae_metadata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supernovae_metadata.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using the data from LSST.\n",
    "\n",
    "\n",
    "Then, we define the wavelength grid as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelength_lsst = {\n",
    "    'u': [3206.34, 4081.51],\n",
    "    'g': [3876.02, 5665.33],\n",
    "    'r': [5377.19, 7055.16],\n",
    "    'i': [6765.77, 8325.05],\n",
    "    'z': [8035.39, 9375.47],\n",
    "    'y': [9089.07, 10915.01]\n",
    "}\n",
    "\n",
    "# Obtain the min and max values of the LSST's wavelength\n",
    "wavelength_grid_min = min([wavelength for wavelength_list in wavelength_lsst.values() for wavelength in wavelength_list])\n",
    "wavelength_grid_max = max([wavelength for wavelength_list in wavelength_lsst.values() for wavelength in wavelength_list])\n",
    "\n",
    "print(f'The min wavelength value to create the grid is: {wavelength_grid_min:.2f} Angstrom')\n",
    "print(f'The max wavelength value to create the grid is: {wavelength_grid_max:.2f} Angstrom')\n",
    "\n",
    "# Number grid's bins\n",
    "nwavelength_grid = 1838\n",
    "\n",
    "# Array equal spacing of wavelengths \n",
    "wavelength_grid_array = np.logspace(np.log10(wavelength_grid_min),np.log10(wavelength_grid_max),nwavelength_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll compute the **Resolution** in velocity terms, $v$. To this, we use the classic Doppler effect in wavelength terms.\n",
    "\n",
    "For a moving source, the relationship between the observed wavelength ($\\lambda^{'}$) and the emitted wavelength ($\\lambda$) is:\n",
    "\n",
    "|Source approaching| Source moves away |\n",
    "|:-:|:-:|\n",
    "|$\\lambda^{'} = \\lambda \\left( 1-\\frac{v}{c}\\right)$ | $\\lambda = \\lambda^{'} \\left( 1+\\frac{v}{c}\\right)$ |\n",
    "\n",
    "Based on the definition of $\\Delta \\lambda = \\lambda_{i} - \\lambda_{i-1}$. Then, average of consecutive wavelengths is:\n",
    "\n",
    "$$\\lambda_{average} = \\frac{\\lambda_{i} + \\lambda_{i-1}}{2}$$\n",
    "\n",
    "Then we obtain the next ratio:\n",
    "\n",
    "$$\\frac{\\Delta \\lambda}{\\lambda_{average}} = \\frac{\\Delta \\lambda}{\\frac{\\lambda_{i} + \\lambda_{i-1}}{2}} = 2 \\times \\frac{\\Delta \\lambda}{\\lambda_{i} + \\lambda_{i-1}}$$\n",
    "\n",
    "At low velocities, the relationship between wavelength change and velocity is approximately linear: \n",
    "\n",
    "$$\\frac{\\Delta \\lambda}{\\lambda_{average}} \\approx \\frac{v}{c}$$\n",
    "\n",
    "Reorganizing\n",
    "\n",
    "$$v \\approx \\frac{\\Delta \\lambda}{\\lambda_{average}} \\times c$$\n",
    "$$v \\approx 2 \\times \\left(\\frac{\\lambda_{i} - \\lambda_{i-1}}{\\lambda_{i} + \\lambda_{i-1}}\\right) \\times c$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to change nwavelength_grid, beacuse we want obtain a velocity equal to 200 km/s\n",
    "CSPEED = 3e5 # km/s\n",
    "dwavelength = wavelength_grid_array[1:] - wavelength_grid_array[:-1]\n",
    "res = 2 * dwavelength/(wavelength_grid_array[1:] + wavelength_grid_array[:-1]) * CSPEED\n",
    "\n",
    "print(f'The mean resolution is: {res.mean():.2f} km/s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we plot the resolution as wavelength function\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(wavelength_grid_array[:-1], res)\n",
    "ax.set_xlabel('Wavelength [Angstrom]')\n",
    "ax.set_ylabel('Resolution [km/s]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading all spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = [files for files in os.listdir(path=PATH_INPUT) if '_raw' not in files]\n",
    "print(f'The total amount of SNs to study will be: {len(all_files)}\\n')\n",
    "\n",
    "print('Some examples are: ...')\n",
    "print(all_files[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try with an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file   = all_files[0]\n",
    "\n",
    "# Obtain the transient name\n",
    "snname = file.split('.')[0]\n",
    "print(f'The working supernova will be {snname} ...')\n",
    "\n",
    "# Reading the spectra data\n",
    "data = pd.read_csv(PATH_INPUT+file)\n",
    "\n",
    "# Converting wavelength to log_wavelength\n",
    "data[\"log10lambda\"] = np.log10(data[\"lambda\"])\n",
    "\n",
    "# fool pandas to make it think log10lambda is days\n",
    "data[\"log10lambda_idx\"] = data[\"log10lambda\"].apply(lambda x: pd.Timedelta(x, 'days'))\n",
    "data.set_index(\"log10lambda_idx\", inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show wavelength resolution in km/s, note that it increases with wavelength.\n",
    "\n",
    "$$ v \\approx \\frac{d\\lambda}{\\lambda} \\times c = d\\ln \\lambda \\times c = d\\log \\lambda \\times \\ln(10) \\times c$$\n",
    "\n",
    "Then, the resolution will be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.abs(data.log10lambda[:-1].values - data.log10lambda[1:].values) * np.log(10) * CSPEED\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = 200 # km/s, using values to show the effect\n",
    "dvsmooth = 2000 # km/s\n",
    "\n",
    "data = data.sort_index()\n",
    "\n",
    "dlog10lambda = dv / CSPEED / np.log(10) * 24 * 3600 # pseudo seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will convolve the observed spectra with a kernel width of half the desired loglambda grid (km/s) and will use a 500 km/s smoothing kernel to define a continuum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = 200 # km/s, using values to show the effect\n",
    "dvsmooth = 2000 # km/s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many seconds would dlog10lambda correspond to? (remember we are fooling pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlog10lambda = dv / CSPEED / np.log(10) * 24 * 3600 # pseudo seconds\n",
    "dlog10lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And dlog10lambdasmooth? (remember we are fooling pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlog10lambdasmooth = dvsmooth / CSPEED / np.log(10) * 24 * 3600 # pseudo seconds\n",
    "dlog10lambdasmooth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot original and smoothed versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "ax.plot(np.array(data[\"lambda\"]), np.array(data.flux_lambda), marker='o', alpha=0.5)\n",
    "ax.plot(np.array(data[\"lambda\"]), data.flux_lambda.rolling(f'{int(dlog10lambda)}s', center=True).mean(), alpha=0.5)\n",
    "ax.plot(data[\"lambda\"], data.flux_lambda.rolling(f'{int(dlog10lambdasmooth)}s', center=True).mean(), c='r', lw=4, alpha=0.5)\n",
    "ax.set_xlabel(r\"$\\lambda$ [A]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get km/s w.r.t. FeII. \n",
    "\n",
    "WARNING: this assumes that the spectra is in rest frame wavelength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeII = 5169\n",
    "data[\"vFeII\"] = (data[\"lambda\"] - FeII) / FeII * CSPEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "ax.plot(data[\"lambda\"], data.flux_lambda, marker='o', alpha=0.5)\n",
    "ax.plot(data[\"lambda\"], data.flux_lambda.rolling(f'{int(dlog10lambda)}s', center=True).mean(), alpha=0.5)\n",
    "ax.plot(data[\"lambda\"], data.flux_lambda.rolling(f'{int(dlog10lambdasmooth)}s', center=True).mean(), c='r', lw=4, alpha=0.5)\n",
    "ax.axvline(FeII)\n",
    "ax.set_xlabel(r\"$\\lambda$ [A]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot dispersion w.r.t. smoothed version (use as empirical error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(data[\"lambda\"], data.flux_lambda - data.flux_lambda.rolling(f'{int(dlog10lambda)}s', center=True).mean())\n",
    "ax.plot(data[\"lambda\"], (data.flux_lambda.rolling(f'{int(dlog10lambda)}s', center=True).mean() - data.flux_lambda.rolling(f'{int(dlog10lambdasmooth)}s', center=True).mean()))\n",
    "delta = (data.flux_lambda.rolling(f'{int(dlog10lambda)}s', center=True).mean() - data.flux_lambda.rolling(f'{int(dlog10lambdasmooth)}s', center=True).mean()).rolling(f'{int(dlog10lambdasmooth)}s', center=True).std()\n",
    "ax.plot(data[\"lambda\"], -delta, c='gray')\n",
    "ax.plot(data[\"lambda\"], delta, c='gray')\n",
    "ax.set_xlabel(r\"$\\lambda$ [A]\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point I will generalizate all the calculations to all supernovae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_flux(data:pd.DataFrame, dv:float = 200, dvsmooth:float = 2000) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    data: pd.DataFrame with the spectra data\n",
    "    dv: velocity \n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    pd.Dataframe mainly flux rolled of the log_lambda\n",
    "    \"\"\"\n",
    "\n",
    "    CSPEED = 3e5 # light_speed in km/s\n",
    "\n",
    "    dlog10lambda = dv / CSPEED / np.log(10) * (24 * 3600) # pseudo seconds\n",
    "    dlog10lambdasmooth = dvsmooth / CSPEED / np.log(10) * 24 * 3600 # pseudo seconds\n",
    "\n",
    "    #data[\"log10lambda\"] = np.log10(data[\"lambda\"])\n",
    "    #data[\"log10lambdasmooth\"] = np.log10(data[\"lambda\"])\n",
    "\n",
    "    # fool pandas to make it think log10lambda is days\n",
    "    #data[\"log10lambda_idx\"] = data[\"log10lambda\"].apply(lambda x: pd.Timedelta(x, 'days'))\n",
    "\n",
    "    result = []\n",
    "    for name, group in data.groupby('mjd'):\n",
    "        group = group.sort_values(by='lambda', ascending=True)\n",
    "        group[\"log10lambda\"] = np.log10(data[\"lambda\"])\n",
    "        group[\"log10lambda_idx\"] = group[\"log10lambda\"].apply(lambda x: pd.Timedelta(x, 'days'))\n",
    "        group = group.set_index('log10lambda_idx')\n",
    "        group[\"flux_log10lambda_rolling\"] = group.flux_lambda.rolling(f'{int(dlog10lambda)}s', center=True).mean()\n",
    "        group[\"flux_log10lambda_rolling_smooth\"] = group.flux_lambda.rolling(f'{int(dlog10lambdasmooth)}s', center=True).mean()\n",
    "        group[\"eflux_log10lambda_rolling\"] = (group.flux_log10lambda_rolling - group.flux_lambda.rolling(f'{int(dlog10lambdasmooth)}s', center=True).mean()).rolling(f'{int(dlog10lambdasmooth)}s', center=True).std()\n",
    "        result.append(group)\n",
    "    data = pd.concat(result)\n",
    "    data.reset_index(inplace=True)\n",
    "    data = data.drop('log10lambda_idx',axis=1)\n",
    "    data\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_interpolated_flux(x, y, lambda_grid:np.array) -> np.array:\n",
    "\n",
    "    f = interpolate.interp1d(x, y, fill_value=np.nan, bounds_error=False)\n",
    "\n",
    "    # computing the new flux in the lambda_grid\n",
    "    flux_new = f(lambda_grid)\n",
    "\n",
    "    return flux_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista para almacenar los resultados\n",
    "def arrange_spectra(sn_name:str,data: pd.DataFrame,oid:int,\n",
    "                    lambda_grid:np.array, nlambda_grid: int) -> pd.DataFrame:\n",
    "    results = []\n",
    "    dlog10lambdasmooth = 2000 / CSPEED / np.log(10) * 24 * 3600\n",
    "    for inst_name, inst_group in data.groupby('instrument'):\n",
    "        for mjd, mjd_group in inst_group.groupby('mjd'):\n",
    "            flux_lambda = obtain_interpolated_flux(x=data['lambda'], y=data['flux_log10lambda_rolling'], lambda_grid=lambda_grid)\n",
    "            flux_lambda_smooth = obtain_interpolated_flux(x=data['lambda'], y=data['flux_log10lambda_rolling_smooth'], lambda_grid=lambda_grid)\n",
    "            eflux_lambda = flux_lambda-flux_lambda_smooth\n",
    "\n",
    "            data_flux = {\n",
    "                'oid': oid,\n",
    "                'snname':sn_name,\n",
    "                'instrument': inst_name,\n",
    "                'mjd': mjd,\n",
    "                'lambda_grid_min': lambda_grid.min(),\n",
    "                'lambda_grid_max': lambda_grid.max(),\n",
    "                'nlambda_grid': nlambda_grid,\n",
    "                'lambda_data_min': mjd_group['lambda'].min(),\n",
    "                'lambda_data_max': mjd_group['lambda'].max(),\n",
    "                #'flux_lambda': mjd_group.flux_log10lambda_rolling.tolist(),\n",
    "                'flux_lambda': flux_lambda,\n",
    "                'flux_lambda_smooth': flux_lambda_smooth,\n",
    "                'e_flux_lambda': mjd_group.eflux_log10lambda_rolling.tolist(),\n",
    "                #'e_flux_lambda': eflux_lambda,\n",
    "                }\n",
    "            results.append(data_flux)\n",
    "\n",
    "            oid += 1\n",
    "\n",
    "    unique_table = pd.DataFrame(results)\n",
    "    return unique_table, oid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = all_files[0]\n",
    "data = pd.read_csv(PATH_INPUT+file)\n",
    "data = smooth_flux(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dataframe = pd.DataFrame()\n",
    "indx_ini = 14000\n",
    "indx_fin = 18000\n",
    "oid = indx_ini\n",
    "for file in all_files[indx_ini:indx_fin]:\n",
    "    try:\n",
    "        sn_name = file.split('.')[0]\n",
    "        data = pd.read_csv(PATH_INPUT+file)\n",
    "        data = smooth_flux(data=data)\n",
    "        result_table, oid = arrange_spectra(sn_name,data,oid, wavelength_grid_array, nwavelength_grid)\n",
    "        master_dataframe = pd.concat([master_dataframe, result_table])\n",
    "        oid = oid\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dataframe = pd.DataFrame()\n",
    "indx_ini = 6000\n",
    "indx_fin = 10000\n",
    "oid = indx_ini\n",
    "for file in all_files[indx_ini:indx_fin]:\n",
    "    sn_name = file.split('.')[0]\n",
    "    data = pd.read_csv(PATH_INPUT+file)\n",
    "    data = smooth_flux(data=data)\n",
    "    result_table, oid = arrange_spectra(sn_name,data,oid, wavelength_grid_array, nwavelength_grid)\n",
    "    master_dataframe = pd.concat([master_dataframe, result_table])\n",
    "    oid = oid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dataframe.to_pickle(f'./master_spectra_table_{indx_ini}_{indx_fin}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dataframe = pd.read_pickle(f'./master_spectra_table_{indx_ini}_{indx_fin}.pkl')\n",
    "master_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dataframe = pd.read_pickle(f'./spectra_wisrep_20240622.pkl')\n",
    "master_dataframe = master_dataframe.drop(master_dataframe.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_data = master_dataframe.iloc[0]\n",
    "test = sn_data['flux_lambda']\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_data = master_dataframe.iloc[0]\n",
    "x = np.logspace(np.log10(sn_data.lambda_grid_min),np.log10(sn_data.lambda_grid_max),sn_data.nlambda_grid)\n",
    "y = sn_data.flux_lambda\n",
    "fig, ax = plt.subplots()\n",
    "sn_data.plot(x=x, y='flux_lambda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(master_dataframe.snname.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import requests \n",
    "credentials_file = \"https://raw.githubusercontent.com/alercebroker/usecases/master/alercereaduser_v4.json\"\n",
    "params = requests.get(credentials_file).json()[\"params\"]\n",
    "conn = psycopg2.connect(dbname=params[\"dbname\"], user=params[\"user\"], host=params[\"host\"], password=params[\"password\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ztf_crossmatch(conn, df, search_radius=1):\n",
    "    '''\n",
    "    conn: connection to database\n",
    "    df: external catalog dataframe (with columns id_source, ra, dec)\n",
    "    search_radius: external radius in arcsec (default=1)\n",
    "\n",
    "    The output is a dataframe with the source id, ra, and dec,\n",
    "    as well as the ALeRCE database meanra, meandec, the crossmatch distance\n",
    "    in degrees and the time of first detection according to the ALeRCE database\n",
    "    '''\n",
    "    \n",
    "    # Preparar el dataframe\n",
    "    objects = []\n",
    "    for _, row in df.iterrows():\n",
    "        objects.append(f\"(\\'{row.id_source}\\', {row.ra}, {row.dec}, \\'{row.redshift}\\', \\'{row.true_label}\\')\")\n",
    "    objects_str = \",\\n\".join(objects)\n",
    "\n",
    "    # Convertir el radio a grados\n",
    "    search_radius = search_radius / 3600\n",
    "\n",
    "    # Preparar el query\n",
    "    query = \"\"\"\n",
    "    WITH catalog (source_id, ra, dec, redshift, true_label) AS (\n",
    "        VALUES\n",
    "            {values}\n",
    "    )\n",
    "    SELECT \n",
    "        c.source_id, c.ra, c.dec, o.oid, o.meanra, o.meandec, q3c_dist(c.ra, c.dec, o.meanra, o.meandec), \n",
    "        o.firstmjd,\n",
    "        c.redshift, c.true_label\n",
    "    FROM object o, catalog c\n",
    "    WHERE\n",
    "        q3c_join(c.ra, c.dec, o.meanra, o.meandec, {radius})\n",
    "    \"\"\"\n",
    "\n",
    "    # Formatear el query final\n",
    "    query_str = query.format(values=objects_str, radius=search_radius)\n",
    "\n",
    "    # Ejecutar el query\n",
    "    try:\n",
    "        matches = pd.read_sql(query_str,conn)\n",
    "        matches[\"q3c_dist\"] = matches.q3c_dist * 3600\n",
    "        matches.rename({\"q3c_dist\": \"dist_arcsec\"}, axis=1, inplace=True)\n",
    "        return matches\n",
    "    except:\n",
    "        print(\"Error accessing the database. Most common causes are timeout errors or wrongly formatted input query.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "supernovae_metadata = pd.read_csv('./wiserep_spectra_metadata.csv',low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Obj. ID</th>\n",
       "      <th>IAU name</th>\n",
       "      <th>Internal name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5875</td>\n",
       "      <td>SN 1999Z</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5875</td>\n",
       "      <td>SN 1999Z</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5875</td>\n",
       "      <td>SN 1999Z</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5875</td>\n",
       "      <td>SN 1999Z</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5875</td>\n",
       "      <td>SN 1999Z</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46446</th>\n",
       "      <td>8896</td>\n",
       "      <td>SN 2016hhv</td>\n",
       "      <td>ATLAS16drn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46447</th>\n",
       "      <td>8896</td>\n",
       "      <td>SN 2016hhv</td>\n",
       "      <td>ATLAS16drn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46448</th>\n",
       "      <td>205</td>\n",
       "      <td>SN 2016hht</td>\n",
       "      <td>PS17aja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46449</th>\n",
       "      <td>427</td>\n",
       "      <td>SN 2016hhj</td>\n",
       "      <td>iPTF16hhj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46450</th>\n",
       "      <td>427</td>\n",
       "      <td>SN 2016hhj</td>\n",
       "      <td>iPTF16hhj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46451 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Obj. ID    IAU name Internal name\n",
       "0         5875    SN 1999Z           NaN\n",
       "1         5875    SN 1999Z           NaN\n",
       "2         5875    SN 1999Z           NaN\n",
       "3         5875    SN 1999Z           NaN\n",
       "4         5875    SN 1999Z           NaN\n",
       "...        ...         ...           ...\n",
       "46446     8896  SN 2016hhv    ATLAS16drn\n",
       "46447     8896  SN 2016hhv    ATLAS16drn\n",
       "46448      205  SN 2016hht       PS17aja\n",
       "46449      427  SN 2016hhj     iPTF16hhj\n",
       "46450      427  SN 2016hhj     iPTF16hhj\n",
       "\n",
       "[46451 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supernovae_cross_names = [\n",
    "    supernovae_metadata['Obj. ID'],\n",
    "    supernovae_metadata['IAU name'], \n",
    "    supernovae_metadata['Internal name/s'].apply(lambda x: str(x).split(',')[0].strip() if pd.notnull(x) else x)\n",
    "]\n",
    "supernovae_cross_names[0].name, supernovae_cross_names[1].name, supernovae_cross_names[2].name = 'Obj. ID', 'IAU name', 'Internal name'\n",
    "supernovae_cross_names = pd.concat(supernovae_cross_names, axis=1)\n",
    "supernovae_cross_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsn = supernovae_metadata[[\"IAU name\", \"Obj. RA\", \"Obj. DEC\", \"Redshift\", \"Obj. Type\"]].rename({\"IAU name\":\"id_source\",'Obj. RA': \"ra\",\"Obj. DEC\":\"dec\", \"Redshift\": \"redshift\", \"Obj. Type\": \"true_label\"},axis=1)\n",
    "results = ztf_crossmatch(conn, dfsn)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_pickle('results_wiserep.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.source_id = results.source_id.replace('nan',np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.dropna()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_supernovae_metadata = supernovae_metadata.set_index('IAU name')\n",
    "test_supernovae_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_supernovae_metadata[results.source_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_pickle('results_wiserep.pkl')\n",
    "new_results = results.drop_duplicates(subset='source_id',keep='first').reset_index(drop=True)\n",
    "new_results = new_results[['oid','source_id','true_label','firstmjd','redshift','ra','dec','meanra','meandec']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_results.to_pickle('ALERCExWiserp20240721_to_AvMW.pkl')\n",
    "new_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Obj. ID</th>\n",
       "      <th>IAU name</th>\n",
       "      <th>Internal name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5875</td>\n",
       "      <td>SN 1999Z</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6732</td>\n",
       "      <td>SN 1999X</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5821</td>\n",
       "      <td>SN 1999U</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1885</td>\n",
       "      <td>SN 1999S</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1545</td>\n",
       "      <td>SN 1999Q</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19924</th>\n",
       "      <td>935</td>\n",
       "      <td>SN 2016hid</td>\n",
       "      <td>PS16exz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19925</th>\n",
       "      <td>3311</td>\n",
       "      <td>SN 2016hhz</td>\n",
       "      <td>Gaia16cae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19926</th>\n",
       "      <td>8896</td>\n",
       "      <td>SN 2016hhv</td>\n",
       "      <td>ATLAS16drn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19927</th>\n",
       "      <td>205</td>\n",
       "      <td>SN 2016hht</td>\n",
       "      <td>PS17aja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19928</th>\n",
       "      <td>427</td>\n",
       "      <td>SN 2016hhj</td>\n",
       "      <td>iPTF16hhj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19929 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Obj. ID    IAU name Internal name\n",
       "0         5875    SN 1999Z           NaN\n",
       "1         6732    SN 1999X           NaN\n",
       "2         5821    SN 1999U           NaN\n",
       "3         1885    SN 1999S           NaN\n",
       "4         1545    SN 1999Q           NaN\n",
       "...        ...         ...           ...\n",
       "19924      935  SN 2016hid       PS16exz\n",
       "19925     3311  SN 2016hhz     Gaia16cae\n",
       "19926     8896  SN 2016hhv    ATLAS16drn\n",
       "19927      205  SN 2016hht       PS17aja\n",
       "19928      427  SN 2016hhj     iPTF16hhj\n",
       "\n",
       "[19929 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_SN_cross_names = supernovae_cross_names.drop_duplicates(subset='Obj. ID',keep='first').reset_index(drop=True)#.set_index('IAU name')\n",
    "new_SN_cross_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_id</th>\n",
       "      <th>IAU name</th>\n",
       "      <th>Internal name</th>\n",
       "      <th>oid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SN1990E</td>\n",
       "      <td>SN 1990E</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZTF22abqtedd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SN2024yr</td>\n",
       "      <td>SN 2024yr</td>\n",
       "      <td>PS24ara</td>\n",
       "      <td>ZTF24aaekmnz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SN2024yq</td>\n",
       "      <td>SN 2024yq</td>\n",
       "      <td>ZTF24aabvtdh</td>\n",
       "      <td>ZTF24aabvtdh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SN2024ym</td>\n",
       "      <td>SN 2024ym</td>\n",
       "      <td>ZTF24aabplhb</td>\n",
       "      <td>ZTF24aabplhb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024xx</td>\n",
       "      <td>2024xx</td>\n",
       "      <td>ATLAS24auj</td>\n",
       "      <td>ZTF24aabuisb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9720</th>\n",
       "      <td>SN2021tgb</td>\n",
       "      <td>SN 2021tgb</td>\n",
       "      <td>ATLAS21bbue</td>\n",
       "      <td>ZTF21abljmmv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9721</th>\n",
       "      <td>SN2021tfx</td>\n",
       "      <td>SN 2021tfx</td>\n",
       "      <td>ZTF21abkrteb</td>\n",
       "      <td>ZTF21abkrteb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9722</th>\n",
       "      <td>SN2016nx</td>\n",
       "      <td>SN 2016nx</td>\n",
       "      <td>PS16pb</td>\n",
       "      <td>ZTF22abntkbk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9723</th>\n",
       "      <td>SN2006nz</td>\n",
       "      <td>SN 2006nz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZTF18abshjwa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9724</th>\n",
       "      <td>SN2016ieq</td>\n",
       "      <td>SN 2016ieq</td>\n",
       "      <td>PS17dne</td>\n",
       "      <td>ZTF19abkaxlf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9725 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      source_id    IAU name Internal name           oid\n",
       "0       SN1990E    SN 1990E           NaN  ZTF22abqtedd\n",
       "1      SN2024yr   SN 2024yr       PS24ara  ZTF24aaekmnz\n",
       "2      SN2024yq   SN 2024yq  ZTF24aabvtdh  ZTF24aabvtdh\n",
       "3      SN2024ym   SN 2024ym  ZTF24aabplhb  ZTF24aabplhb\n",
       "4        2024xx      2024xx    ATLAS24auj  ZTF24aabuisb\n",
       "...         ...         ...           ...           ...\n",
       "9720  SN2021tgb  SN 2021tgb   ATLAS21bbue  ZTF21abljmmv\n",
       "9721  SN2021tfx  SN 2021tfx  ZTF21abkrteb  ZTF21abkrteb\n",
       "9722   SN2016nx   SN 2016nx        PS16pb  ZTF22abntkbk\n",
       "9723   SN2006nz   SN 2006nz           NaN  ZTF18abshjwa\n",
       "9724  SN2016ieq  SN 2016ieq       PS17dne  ZTF19abkaxlf\n",
       "\n",
       "[9725 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossmatched = pd.merge(left=new_results, right=new_SN_cross_names, how='inner', left_on='source_id', right_on='IAU name')[['source_id','IAU name','Internal name','oid']]\n",
    "crossmatched.source_id = crossmatched.source_id.apply(lambda x: x.replace(' ',''))\n",
    "crossmatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dataframe = pd.read_pickle('master_spectra_table_main.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['oid', 'snname', 'instrument', 'mjd', 'lambda_grid_min',\n",
       "       'lambda_grid_max', 'nlambda_grid', 'lambda_data_min', 'lambda_data_max',\n",
       "       'flux_lambda', 'flux_lambda_smooth', 'e_flux_lambda'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oid</th>\n",
       "      <th>snname</th>\n",
       "      <th>instrument</th>\n",
       "      <th>mjd</th>\n",
       "      <th>lambda_grid_min</th>\n",
       "      <th>lambda_grid_max</th>\n",
       "      <th>nlambda_grid</th>\n",
       "      <th>lambda_data_min</th>\n",
       "      <th>lambda_data_max</th>\n",
       "      <th>flux_lambda</th>\n",
       "      <th>flux_lambda_smooth</th>\n",
       "      <th>e_flux_lambda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SN2022pru</td>\n",
       "      <td>Lick-3m_KAST</td>\n",
       "      <td>59788.215972</td>\n",
       "      <td>3206.34</td>\n",
       "      <td>10915.01</td>\n",
       "      <td>1838</td>\n",
       "      <td>3504.527001</td>\n",
       "      <td>10393.131809</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>[0.21898910712578754, 0.1979063641775711, 0.18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SN2022ytx</td>\n",
       "      <td>LT_SPRAT</td>\n",
       "      <td>59886.979942</td>\n",
       "      <td>3206.34</td>\n",
       "      <td>10915.01</td>\n",
       "      <td>1838</td>\n",
       "      <td>4047.600000</td>\n",
       "      <td>7994.400000</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>[0.2513674591903225, 0.2889965846960843, 0.113...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SN2022ytx</td>\n",
       "      <td>LT_SPRAT</td>\n",
       "      <td>59907.898866</td>\n",
       "      <td>3206.34</td>\n",
       "      <td>10915.01</td>\n",
       "      <td>1838</td>\n",
       "      <td>4066.000000</td>\n",
       "      <td>7994.400000</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>[0.12373791200226614, 0.10053409443820172, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SNLS-07D3do</td>\n",
       "      <td>Gemini-N_GMOS</td>\n",
       "      <td>54200.000000</td>\n",
       "      <td>3206.34</td>\n",
       "      <td>10915.01</td>\n",
       "      <td>1838</td>\n",
       "      <td>5104.440000</td>\n",
       "      <td>9352.610000</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>[1.5368479599410815e-18, 1.394673534721022e-18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SNLS-07D3do</td>\n",
       "      <td>Gemini-N_GMOS</td>\n",
       "      <td>54200.000000</td>\n",
       "      <td>3206.34</td>\n",
       "      <td>10915.01</td>\n",
       "      <td>1838</td>\n",
       "      <td>5048.346943</td>\n",
       "      <td>9355.344674</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>[1.797687567656606e-18, 1.8212694935234954e-18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25948</th>\n",
       "      <td>ZTF21aanvncv</td>\n",
       "      <td>SN2021efd</td>\n",
       "      <td>NOT_ALFOSC</td>\n",
       "      <td>59277.154005</td>\n",
       "      <td>3206.34</td>\n",
       "      <td>10915.01</td>\n",
       "      <td>1838</td>\n",
       "      <td>3399.900000</td>\n",
       "      <td>9675.765091</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>[9.342260744967236e-17, 8.189705456575588e-17,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25949</th>\n",
       "      <td>ZTF20aauoipy</td>\n",
       "      <td>SN2020etk</td>\n",
       "      <td>LT_SPRAT</td>\n",
       "      <td>58970.392257</td>\n",
       "      <td>3206.34</td>\n",
       "      <td>10915.01</td>\n",
       "      <td>1838</td>\n",
       "      <td>4020.000000</td>\n",
       "      <td>7994.400000</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>[0.44910237291240807, 0.34667301062037215, 0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25950</th>\n",
       "      <td>ZTF20abkwvgc</td>\n",
       "      <td>SN2020oqp</td>\n",
       "      <td>P60_SEDM</td>\n",
       "      <td>59047.214850</td>\n",
       "      <td>3206.34</td>\n",
       "      <td>10915.01</td>\n",
       "      <td>1838</td>\n",
       "      <td>3776.700000</td>\n",
       "      <td>9223.300000</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25951</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SDSS-SN-082</td>\n",
       "      <td>Sloan_SDSS-Spec</td>\n",
       "      <td>54233.000000</td>\n",
       "      <td>3206.34</td>\n",
       "      <td>10915.01</td>\n",
       "      <td>1838</td>\n",
       "      <td>3822.082000</td>\n",
       "      <td>9189.671900</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>[1.1354836383504999e-17, 1.118140824452677e-17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25952</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PTF12edl</td>\n",
       "      <td>UH88_SNIFS</td>\n",
       "      <td>56062.519051</td>\n",
       "      <td>3206.34</td>\n",
       "      <td>10915.01</td>\n",
       "      <td>1838</td>\n",
       "      <td>3301.060059</td>\n",
       "      <td>9701.229988</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>[2.8574681109790546e-17, 3.0431750187716936e-1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25953 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                oid       snname       instrument           mjd  \\\n",
       "0               NaN    SN2022pru     Lick-3m_KAST  59788.215972   \n",
       "1               NaN    SN2022ytx         LT_SPRAT  59886.979942   \n",
       "2               NaN    SN2022ytx         LT_SPRAT  59907.898866   \n",
       "3               NaN  SNLS-07D3do    Gemini-N_GMOS  54200.000000   \n",
       "4               NaN  SNLS-07D3do    Gemini-N_GMOS  54200.000000   \n",
       "...             ...          ...              ...           ...   \n",
       "25948  ZTF21aanvncv    SN2021efd       NOT_ALFOSC  59277.154005   \n",
       "25949  ZTF20aauoipy    SN2020etk         LT_SPRAT  58970.392257   \n",
       "25950  ZTF20abkwvgc    SN2020oqp         P60_SEDM  59047.214850   \n",
       "25951           NaN  SDSS-SN-082  Sloan_SDSS-Spec  54233.000000   \n",
       "25952           NaN     PTF12edl       UH88_SNIFS  56062.519051   \n",
       "\n",
       "       lambda_grid_min  lambda_grid_max  nlambda_grid  lambda_data_min  \\\n",
       "0              3206.34         10915.01          1838      3504.527001   \n",
       "1              3206.34         10915.01          1838      4047.600000   \n",
       "2              3206.34         10915.01          1838      4066.000000   \n",
       "3              3206.34         10915.01          1838      5104.440000   \n",
       "4              3206.34         10915.01          1838      5048.346943   \n",
       "...                ...              ...           ...              ...   \n",
       "25948          3206.34         10915.01          1838      3399.900000   \n",
       "25949          3206.34         10915.01          1838      4020.000000   \n",
       "25950          3206.34         10915.01          1838      3776.700000   \n",
       "25951          3206.34         10915.01          1838      3822.082000   \n",
       "25952          3206.34         10915.01          1838      3301.060059   \n",
       "\n",
       "       lambda_data_max                                        flux_lambda  \\\n",
       "0         10393.131809  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
       "1          7994.400000  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
       "2          7994.400000  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
       "3          9352.610000  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
       "4          9355.344674  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
       "...                ...                                                ...   \n",
       "25948      9675.765091  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
       "25949      7994.400000  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
       "25950      9223.300000  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
       "25951      9189.671900  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
       "25952      9701.229988  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
       "\n",
       "                                      flux_lambda_smooth  \\\n",
       "0      [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
       "1      [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
       "2      [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
       "3      [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
       "4      [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
       "...                                                  ...   \n",
       "25948  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
       "25949  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
       "25950  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
       "25951  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
       "25952  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
       "\n",
       "                                           e_flux_lambda  \n",
       "0      [0.21898910712578754, 0.1979063641775711, 0.18...  \n",
       "1      [0.2513674591903225, 0.2889965846960843, 0.113...  \n",
       "2      [0.12373791200226614, 0.10053409443820172, 0.1...  \n",
       "3      [1.5368479599410815e-18, 1.394673534721022e-18...  \n",
       "4      [1.797687567656606e-18, 1.8212694935234954e-18...  \n",
       "...                                                  ...  \n",
       "25948  [9.342260744967236e-17, 8.189705456575588e-17,...  \n",
       "25949  [0.44910237291240807, 0.34667301062037215, 0.2...  \n",
       "25950  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...  \n",
       "25951  [1.1354836383504999e-17, 1.118140824452677e-17...  \n",
       "25952  [2.8574681109790546e-17, 3.0431750187716936e-1...  \n",
       "\n",
       "[25953 rows x 12 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_master = master_dataframe.merge(crossmatched[['source_id','oid']], how='left', left_on='snname',right_on='source_id').drop('source_id', axis=1)\n",
    "cross_master = cross_master.merge(crossmatched[['Internal name','oid']], how='left', left_on='snname',right_on='Internal name', suffixes=('', '_new')).drop('Internal name',axis=1)\n",
    "cross_master['oid'] = cross_master['oid_y']\n",
    "cross_master = cross_master.drop(['oid_x','oid_y'],axis=1)\n",
    "cross_master = cross_master[['oid','snname', 'instrument', 'mjd', 'lambda_grid_min', 'lambda_grid_max',\n",
    "      'nlambda_grid', 'lambda_data_min', 'lambda_data_max', 'flux_lambda', 'flux_lambda_smooth',\n",
    "      'e_flux_lambda']]\n",
    "cross_master.instrument = cross_master.instrument.apply(lambda x: ('_').join(x.split('_')[0:2])) \n",
    "cross_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_master.to_pickle('spectra_ALeRCE20240801_x_wisrep_20240622.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hola = pd.read_pickle('spectra_ALeRCE20240630_x_wisrep_20240622.pkl')\n",
    "hola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_instrument = data.groupby('instrument')\n",
    "data_instrument\n",
    "for int, group in data_instrument:\n",
    "    group.to_csv(f'{sn_name}_{int}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_name = 'SN2006dv.dat'\n",
    "data = pd.read_csv('./data/spectra/'+sn_name)\n",
    "\n",
    "x_lambda_array = data['lambda']\n",
    "y = data['flux_lambda']\n",
    "f = interpolate.interp1d(x_lambda_array, y, fill_value=np.nan)\n",
    "\n",
    "def plotobject(df,sn_name, inst_name):\n",
    "  fig, ax = plt.subplots(figsize=(10, 6))\n",
    "  for mjd_date, group_df in df.groupby('mjd'):\n",
    "       ax.plot(group_df['lambda'], group_df['flux_lambda'], label=f'{mjd_date:.2f} MJD')\n",
    "  ax.legend(frameon=False)\n",
    "  ax.set_xlabel('Lambda')\n",
    "  ax.set_ylabel('Flux_lambda')\n",
    "  ax.set_title('Supernova:'+sn_name.split(\".\")[0]+\" | Instrument: \"+inst_name.split(\".\")[0])\n",
    "  \n",
    "\n",
    "data_instrument = data.groupby('instrument')\n",
    "for inst_name, group in data_instrument:\n",
    "    plotobject(group, sn_name=sn_name, inst_name=inst_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dataframe = pd.DataFrame()\n",
    "indx_ini = 0\n",
    "indx_fin = 12000\n",
    "oid = 11000 - 1\n",
    "for file in all_files[indx_ini:indx_fin]:\n",
    "    sn_name = file.split('.')[0]\n",
    "    data = pd.read_csv(PATH_input+file)\n",
    "    result_table, oid = arrange_spectra(sn_name=sn_name, data=data,oid=oid)\n",
    "    master_dataframe = pd.concat([master_dataframe, result_table])\n",
    "    oid = oid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dataframe.to_csv(f'./master_spectra_table_{indx_ini}_{indx_fin}.cvs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dataframe_list = [file for file in os.listdir(path='./') if file.startswith(\"master_spectra\")]\n",
    "master_dataframe_list.remove('master_spectra_wiserep.csv')\n",
    "#master_dataframe_list.remove('master_spectra_table_main.pkl')\n",
    "master_dataframe_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dataframe = pd.DataFrame()\n",
    "for master_spectra_table in master_dataframe_list:\n",
    "    data = pd.read_pickle(master_spectra_table)\n",
    "    master_dataframe = pd.concat([master_dataframe, data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dataframe.to_pickle(f'./master_spectra_table_main.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dataframe = pd.read_pickle('master_spectra_table_main.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dataframe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dataframe = pd.read_csv('master_spectra_table_main.cvs')\n",
    "master_dataframe = master_dataframe.drop(['Unnamed: 0.1','Unnamed: 0'],axis=1)\n",
    "master_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_dataframe = master_dataframe[master_dataframe['snname'] == 'SN2021rfs']\n",
    "#sn_dataframe = sn_dataframe[0]\n",
    "sn_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_name = 'SN2021rfs.dat'\n",
    "data = pd.read_csv('./data/spectra/'+sn_name)\n",
    "\n",
    "def plotobject(df,sn_name, inst_name):\n",
    "  fig, ax = plt.subplots(figsize=(10, 6))\n",
    "  for mjd_date, group_df in df.groupby('mjd'):\n",
    "       ax.plot(group_df['lambda'], group_df['flux_lambda'], label=f'{mjd_date:.2f} MJD')\n",
    "  ax.legend(frameon=False)\n",
    "  ax.set_xlabel('Lambda')\n",
    "  ax.set_ylabel('Flux_lambda')\n",
    "  ax.set_title('Supernova:'+sn_name.split(\".\")[0]+\" | Instrument: \"+inst_name.split(\".\")[0])\n",
    "  \n",
    "\n",
    "data_instrument = data.groupby('instrument')\n",
    "for inst_name, group in data_instrument:\n",
    "    plotobject(group, sn_name=sn_name, inst_name=inst_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sn_dataframe.plot(x='flux_lambda',y=lambda_grid_lenght)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing AstroDash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetLayer(object):\n",
    "    def __init__(self, N, ntypes, imWidth, imWidthReduc):\n",
    "        self.N = N\n",
    "        self.ntypes = ntypes\n",
    "        self.imWidth = imWidth\n",
    "        self.imWidthReduc = imWidthReduc\n",
    "\n",
    "    def build_layer(self, prevHPool, prevNumFeatures, numFeatures):\n",
    "        W_conv = self._weight_variable([5, 5, prevNumFeatures, numFeatures])\n",
    "        b_conv = self._bias_variable([numFeatures])\n",
    "        h_conv = tf.nn.relu(self._conv2d(prevHPool, W_conv) + b_conv)\n",
    "        h_pool = self._max_pool_2x2(h_conv)\n",
    "        # print(h_pool)\n",
    "\n",
    "        return h_pool\n",
    "\n",
    "    def connect_layers(self, h_pool, numFeatures, layerNum):\n",
    "        W_fc = self._weight_variable(\n",
    "            [int(self.imWidthReduc / layerNum * self.imWidthReduc / layerNum * numFeatures), 1024])\n",
    "        b_fc = self._bias_variable([1024])\n",
    "        h_pool_flat = tf.reshape(h_pool,\n",
    "                                 [-1, int(self.imWidthReduc / layerNum * self.imWidthReduc / layerNum * numFeatures)])\n",
    "        h_fc = tf.nn.relu(tf.matmul(h_pool_flat, W_fc) + b_fc)\n",
    "\n",
    "        return h_fc\n",
    "\n",
    "    def dropout(self, h_fc):\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        h_fc_drop = tf.nn.dropout(h_fc, keep_prob)\n",
    "\n",
    "        return keep_prob, h_fc_drop\n",
    "\n",
    "    def readout_layer(self):\n",
    "        W_fc = self._weight_variable([1024, self.ntypes])\n",
    "        b_fc = self._bias_variable([self.ntypes])\n",
    "\n",
    "        return W_fc, b_fc\n",
    "\n",
    "    def _weight_variable(self, shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def _bias_variable(self, shape):\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def _conv2d(self, x, W):\n",
    "        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "    def _max_pool_2x2(self, x):\n",
    "        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "len([file for file in os.listdir(path='./data/spectra_wiserep_raws')])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
