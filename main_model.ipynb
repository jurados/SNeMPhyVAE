{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Settings by default\n",
    "SIDEREAL_SCALE = 86400. / 86164.0905 # days per sidereal year\n",
    "TIME_WINDOW = 300\n",
    "TIME_PAD = 100\n",
    "ERROR_FLOOR = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mag_to_flux(light_curves):\n",
    "    # Iterar sobre cada curva de luz agrupada por 'oid'\n",
    "    for id_light_curve, light_curve in light_curves.groupby(by='oid'):\n",
    "        \n",
    "        light_curves.loc[light_curve.index, 'flux'] = 10**(-0.4 * (light_curve['magpsf']+48.60))\n",
    "        light_curves.loc[light_curve.index, 'fluxerr'] = 10**(-0.4 * (light_curve['sigmapsf']+48.60))\n",
    "\n",
    "    light_curves = light_curves.drop(columns=['magpsf', 'sigmapsf'])\n",
    "\n",
    "    return light_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossmatch_object_alerce(alerce_lc: pd.DataFrame, object: pd.DataFrame) -> pd.DataFrame:\n",
    "    lightcurves = pd.merge(left=alerce_lc, right=object,\n",
    "                       on='oid')\n",
    "    return lightcurves\n",
    "\n",
    "def process_light_curve_parsnip(ligth_curve):\n",
    "\n",
    "    new_light_curve = ligth_curve.copy()\n",
    "\n",
    "    SIDEREAL_SCALE = 86400. / 86164.0905\n",
    "\n",
    "    time = ligth_curve['mjd'].to_numpy()\n",
    "    sidereal_time = time * SIDEREAL_SCALE\n",
    "\n",
    "    # Initial guess of the phase. Round everything to 0.1 days, and find the decimal\n",
    "    # that has the largest count.\n",
    "    mode, count = scipy.stats.mode(np.round(sidereal_time % 1 + 0.05, 1), keepdims=True)\n",
    "    guess_offset = mode[0] - 0.05\n",
    "\n",
    "    # Shift everything by the guessed offset\n",
    "    guess_shift_time = sidereal_time - guess_offset\n",
    "\n",
    "    # Do a proper estimate of the offset.\n",
    "    sidereal_offset = guess_offset + np.median((guess_shift_time + 0.5) % 1) - 0.5\n",
    "\n",
    "    # Shift everything by the final offset estimate.\n",
    "    shift_time = sidereal_time - sidereal_offset\n",
    "\n",
    "    # Selecting the \n",
    "    s2n = ligth_curve['magpsf'] / ligth_curve['sigmapsf']\n",
    "    s2n_mask = np.argsort(s2n)[-5:]\n",
    "\n",
    "    cut_times = shift_time[s2n_mask]\n",
    "\n",
    "    max_time = np.round(np.median(cut_times))\n",
    "\n",
    "    # Convert back to a reference time in the original units. This reference time\n",
    "    # corresponds to the reference of the grid in sidereal time.\n",
    "    reference_time = ((max_time + sidereal_offset) / SIDEREAL_SCALE)\n",
    "    grid_times = (time - reference_time) * SIDEREAL_SCALE\n",
    "    time_indices = np.round(grid_times).astype(int) + 300 // 2 # 300 days\n",
    "    time_mask = (\n",
    "        (time_indices >= -100)\n",
    "        & (time_indices < 300 + 100)\n",
    "    )\n",
    "    new_light_curve['grid_time'] = grid_times\n",
    "    new_light_curve['time_index'] = time_indices\n",
    "    new_light_curve = new_light_curve[time_mask]\n",
    "\n",
    "    return new_light_curve  \n",
    "\n",
    "def _get_data(light_curves):\n",
    "    device = 'cpu'  \n",
    "    redshifts = []\n",
    "    compare_data = []\n",
    "    \n",
    "    error_floor = 0.01\n",
    "    # Build a grid for the input\n",
    "    # The first grid is created for saved the data\n",
    "    # The second grid is created for save the weights that will be used\n",
    "    # on the loss_function\n",
    "    try:\n",
    "        len_light_curves = len(light_curves.oid.unique())\n",
    "    except:\n",
    "        len_light_curves = len(light_curves)\n",
    "    #print(len_light_curves)\n",
    "    grid_flux    = np.zeros((len_light_curves,1,300))\n",
    "    grid_weights = np.zeros_like(grid_flux) \n",
    "\n",
    "    # Iterate over each unique object ID (oid)\n",
    "    try:\n",
    "        for idx, (oid, light_curve) in enumerate(light_curves.groupby('oid')):\n",
    "            redshifts.append(0.01)\n",
    "            print(light_curve)\n",
    "            # Mask observations outside the window\n",
    "            mask = (light_curve['time_index'] >= 0) & (light_curve['time_index'] < 300)\n",
    "            light_curve = light_curve[mask]\n",
    "\n",
    "            # Calculate weights\n",
    "            weights = 1 / (light_curve['sigmapsf']**2 + error_floor**2)\n",
    "\n",
    "            # Fill in the input arrays\n",
    "            grid_flux[idx, 0, light_curve['time_index']] = light_curve['magpsf']\n",
    "            grid_weights[idx, 0, light_curve['time_index']] = error_floor**2 * weights\n",
    "\n",
    "\n",
    "            obj_compare_data = torch.FloatTensor(np.vstack([\n",
    "                light_curve['grid_time'],\n",
    "                light_curve['magpsf'],\n",
    "                light_curve['sigmapsf'],\n",
    "                weights,\n",
    "            ]))\n",
    "            compare_data.append(obj_compare_data.T)\n",
    "    except:\n",
    "        print('No paso con oids')\n",
    "        pass\n",
    "    try:\n",
    "        for idx, light_curve in enumerate(light_curves):\n",
    "                redshifts.append(0.01)\n",
    "                #print(light_curve)\n",
    "                light_curve = pd.DataFrame(light_curve[list(light_curve.keys())[0]])\n",
    "                #print(light_curve)\n",
    "\n",
    "                # Mask observations outside the window\n",
    "                mask = (light_curve['time_index'] >= 0) & (light_curve['time_index'] < 300)\n",
    "                light_curve = light_curve[mask]\n",
    "\n",
    "                # Calculate weights\n",
    "                weights = 1 / (light_curve['sigmapsf']**2 + error_floor**2)\n",
    "\n",
    "                # Fill in the input arrays\n",
    "                grid_flux[idx, 0, light_curve['time_index']] = light_curve['magpsf']\n",
    "                grid_weights[idx, 0, light_curve['time_index']] = error_floor**2 * weights\n",
    "\n",
    "\n",
    "                obj_compare_data = torch.FloatTensor(np.vstack([\n",
    "                    light_curve['grid_time'],\n",
    "                    light_curve['magpsf'],\n",
    "                    light_curve['sigmapsf'],\n",
    "                    weights,\n",
    "                ]))\n",
    "                compare_data.append(obj_compare_data.T)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    redshifts = np.array(redshifts)\n",
    "    extra_input_data = [redshifts]\n",
    "\n",
    "    input_data = np.concatenate(\n",
    "            [i[:, None, None].repeat(300, axis=2) for i in extra_input_data]\n",
    "            + [grid_flux, grid_weights],\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "    input_data = torch.FloatTensor(input_data).to(device)\n",
    "    redshifts = torch.FloatTensor(redshifts).to(device)\n",
    "\n",
    "    # Pad all of the compare data to have the same shape.\n",
    "    compare_data = nn.utils.rnn.pad_sequence(compare_data, batch_first=True)\n",
    "    compare_data = compare_data.permute(0, 2, 1)\n",
    "    compare_data = compare_data.to(device)\n",
    "\n",
    "    data = {\n",
    "        'input_data': input_data,\n",
    "        'compare_data': compare_data,\n",
    "        'redshift': redshifts,\n",
    "    }\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_light_curve(light_curve, oid:any = None):\n",
    "\n",
    "    time = light_curve['mjd'].to_numpy()\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    try:\n",
    "      mag  = light_curve['magpsf'].to_numpy()\n",
    "      ax.plot(time,mag,'o')\n",
    "      ax.set_ylim(ax.get_ylim()[::-1])\n",
    "      ax.set_ylabel('Apparent magnitude')\n",
    "    except:\n",
    "      flux = light_curve['flux'].to_numpy()\n",
    "      ax.plot(time,flux,'o')\n",
    "      ax.set_ylabel(r'Flux erg s−1 cm−2 Hz−1')\n",
    "    ax.set_xlabel('MJD')\n",
    "\n",
    "\n",
    "    if oid != None:\n",
    "        ax.set_title(f'oid: {oid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_table = pd.read_pickle('~/Supernovae_DeepLearning/object_ZTF_ALeRCE_19052024.pkl')\n",
    "print(object_table)\n",
    "print('\\nNumber of Different Objects in object_table:', len(object_table.oid.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightcurves_alercextns = pd.read_pickle('/home/jurados/Supernovae_DeepLearning/data/lightcurves/lcs_transients_20240517.pkl')\n",
    "print(lightcurves_alercextns)\n",
    "print('\\nNumber of Different Objects in lightcurves_alercextns:', len(lightcurves_alercextns.oid.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I realized a crossmatch between all lightcurves_alercextns and\n",
    "# the object table\n",
    "lightcurves = crossmatch_object_alerce(lightcurves_alercextns, object_table)\n",
    "print(lightcurves)\n",
    "print('\\nNumber of Different Objects in lightcurves (Crossmatch):', len(lightcurves.oid.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightcurves = lightcurves[lightcurves.fid == 1]\n",
    "lightcurves.oid.unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The oid of transients with maximum length is:', lightcurves.groupby(by='oid')['mjd'].size().idxmax())\n",
    "print('The maximum length of observation is:', lightcurves.groupby(by='oid')['mjd'].size().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_unique = lightcurves.oid.unique()\n",
    "selected_unique_values = np.random.choice(sn_unique, int(0.8 * len(sn_unique)), replace=False)\n",
    "selected_unique_values_test = np.random.choice(sn_unique, int(0.2 * len(sn_unique)), replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = lightcurves[lightcurves.oid.isin(selected_unique_values)]\n",
    "test_data = lightcurves[lightcurves.oid.isin(selected_unique_values_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The oid of transients with maximum length is:', train_data.groupby(by='oid')['mjd'].size().idxmax())\n",
    "print('The maximum length of observation is:', train_data.groupby(by='oid')['mjd'].size().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = train_data.copy()\n",
    "test = test[['oid','mjd','magpsf']]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_light_curve = train_data[train_data.oid == train_data.oid.unique()[3]]\n",
    "one_light_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_light_curve(one_light_curve, one_light_curve.oid.unique()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next Syntethic model is based on the papers of [Olivares et al. 2010](https://ui.adsabs.harvard.edu/abs/2010ApJ...715..833O/abstract).\n",
    "\n",
    "This synthetic model created light-curves based on the next functions:\n",
    "\n",
    "$$ f_{\\text{DF}} = \\frac{-a_0}{1+\\exp\\left( (t - t_{\\text{PT}})\\right) / w_0}$$\n",
    "$$ l(t) = p_o(t-t_{\\text{PT}}) + m_0$$\n",
    "$$ g(t) = -P e^{\\left(\\frac{t-Q}{R}\\right)^{2}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticLigthCurve:\n",
    "    def __init__(self,tpt,a0,w0,p0,m0,P,Q,R):\n",
    "        self.a0 = a0   # height of light_curve \n",
    "        self.w0 = w0   # width of the transition phase\n",
    "        self.tpt = tpt # middle of the transition\n",
    "        self.p0 = p0   # slope of the radioactive tail\n",
    "        self.m0 = m0   # the zero point in the magnitude\n",
    "        self.P = P     # height of the Gaussian peak\n",
    "        self.Q = Q     # center of the Gaussian function\n",
    "        self.R = R     # width of the Gaussian function\n",
    "\n",
    "    def olivares(self,t):\n",
    "        f_fd = -self.a0/(1+np.exp((t-self.tpt)/self.w0))\n",
    "        f_ld = self.p0*(t-self.tpt)+self.m0\n",
    "        f_gs =  -self.P * np.exp(-((t-self.Q)/self.R)**2)\n",
    "        f = f_fd + f_ld  + f_gs\n",
    "        return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "po_v = [95,1.744,3.602,0.008,14.482,1.675,20.148,-15.984] # tpt, a0, w0, p0, m0, P, Q, R\n",
    "po_r = [88.948, 1.584, 4.485, 0.005, 13.759, 1.528, 101.390, -17.934]\n",
    "fig, ax = plt.subplots()\n",
    "synthetic_lc_v = SyntheticLigthCurve(po_v[0],po_v[1],po_v[2],po_v[3],po_v[4],po_v[5],po_v[6],po_v[7])\n",
    "synthetic_lc_r = SyntheticLigthCurve(po_r[0],po_r[1],po_r[2],po_r[3],po_r[4],po_r[5],po_r[6],po_r[7])\n",
    "time = np.linspace(50,150,300)\n",
    "ax.plot(time, synthetic_lc_v.olivares(time))\n",
    "ax.plot(time, synthetic_lc_r.olivares(time))\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Time [days]')\n",
    "ax.set_ylabel('Apparent Magnitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = [91.026,1.744,3.602,0.008,14.482,1.675,102.148,-15.984] # tpt, a0, w0, p0, m0, P, Q, R\n",
    "\n",
    "N = 10000\n",
    "time_length = 300 # days\n",
    "\n",
    "oid = []\n",
    "mjd = []\n",
    "mag = []\n",
    "\n",
    "for n in range(N):\n",
    "    increase_random = np.random.random()\n",
    "    time = np.linspace(50, 150, 50)\n",
    "    po = [p0[0] + increase_random, p0[1] + increase_random, p0[2] + 0.01*increase_random, p0[3], p0[4] + 0.01*increase_random,\n",
    "          p0[5] + 0.01*increase_random, p0[6] + 0.01*increase_random, p0[7] + 0.01*increase_random]\n",
    "    #print(po)\n",
    "    synthetic_lc = SyntheticLigthCurve(po[0],po[1],po[2],po[3],po[4],po[5],po[6],po[7])\n",
    "    f = synthetic_lc.olivares(time)\n",
    "    mjd.append(np.array(time))\n",
    "    mag.append(np.array(f))\n",
    "\n",
    "# Crear una lista vacía para almacenar los datos\n",
    "train_data_synthetic = []\n",
    "test_data_synthetic  = []\n",
    "\n",
    "# Iterar sobre cada conjunto de tiempo y brillo\n",
    "for i in range(len(mjd)):\n",
    "    time_set = mjd[i]\n",
    "    brightness_set = mag[i]\n",
    "    code = 'ZTF' + str(i + 1)  # Código de pertenencia (del 1 al 8000)\n",
    "    for time, brightness in zip(time_set, brightness_set):\n",
    "        if i+1 <= 8_000:\n",
    "          train_data_synthetic.append([code, time, brightness])\n",
    "        else:\n",
    "          test_data_synthetic.append([code, time, brightness])\n",
    "\n",
    "# Crear el DataFrame\n",
    "train_data_synthetic = pd.DataFrame(train_data_synthetic, columns=['oid','mjd', 'magpsf'])\n",
    "test_data_synthetic = pd.DataFrame(test_data_synthetic, columns=['oid','mjd', 'magpsf'])\n",
    "train_data_synthetic['sigmapsf'] = 0.001\n",
    "test_data_synthetic['sigmapsf'] = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oid_idx = 0\n",
    "plot_light_curve(train_data_synthetic[train_data_synthetic.oid == train_data_synthetic.oid.unique()[oid_idx]], oid=train_data_synthetic.oid.unique()[oid_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_synthetic = mag_to_flux(train_data_synthetic)\n",
    "test_data_synthetic  = mag_to_flux(test_data_synthetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oid_idx = 0\n",
    "plot_light_curve(train_data_synthetic[train_data_synthetic.oid == train_data_synthetic.oid.unique()[oid_idx]], oid=train_data_synthetic.oid.unique()[oid_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_synthetic.head())\n",
    "print('\\nShape train_data_synthethic:', train_data_synthetic.shape)\n",
    "print('Length train_data_synthethic:', len(train_data_synthetic))\n",
    "print()\n",
    "print(test_data_synthetic.head())\n",
    "print('\\nShape test_data_synthethic:', test_data_synthetic.shape)\n",
    "print('Length test_data_synthethic:', len(test_data_synthetic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_light_curves(light_curves):\n",
    "    \"\"\" Create a new data set adding the grid_time and\n",
    "    time_index columns\n",
    "    \"\"\"\n",
    "\n",
    "    new_light_curves = pd.DataFrame()\n",
    "    for id_group, group in light_curves.groupby('oid'):\n",
    "        light_curve_processed = process_light_curve_parsnip(group)\n",
    "        new_light_curves = pd.concat([new_light_curves, light_curve_processed])\n",
    "\n",
    "    return new_light_curves\n",
    "\n",
    "\n",
    "train_data_synthetic = new_light_curves(train_data_synthetic)\n",
    "test_data_synthetic  = new_light_curves(test_data_synthetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_synthetic.head())\n",
    "print('\\nShape train_data_synthethic:', train_data_synthetic.shape)\n",
    "print('Length train_data_synthethic:', len(train_data_synthetic))\n",
    "print()\n",
    "print(test_data_synthetic.head())\n",
    "print('\\nShape test_data_synthethic:', test_data_synthetic.shape)\n",
    "print('Length test_data_synthethic:', len(test_data_synthetic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix_train = _get_data(train_data_synthetic)\n",
    "#matrix_test = _get_data(test_data_synthetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix_train['compare_data'][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightCurveDataset():\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        # Agrupar por 'oid' y almacenar los grupos\n",
    "        self.groups = dataframe.groupby('oid')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.groups)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Obtener el 'oid' por índice (esto devuelve una tupla: (oid, dataframe))\n",
    "        oid, group = list(self.groups)[idx]\n",
    "\n",
    "        # Convertir las columnas 'tiempo' y 'mag' en listas\n",
    "        #mjd = torch.tensor(group['mjd'].values, dtype=torch.float32).to(device)\n",
    "        #magpsf = torch.tensor(group['magpsf'].values, dtype=torch.float32).to(device)\n",
    "        #sigmapsf = torch.tensor(group['sigmapsf'].values, dtype=torch.float32).to(device)\n",
    "        #grid_time = torch.tensor(group['grid_time'].values, dtype=torch.float32).to(device)\n",
    "        #time_index = torch.tensor(group['time_index'].values, dtype=torch.float32).to(device)\n",
    "\n",
    "        mjd = group['mjd'].to_numpy()\n",
    "        flux = group['flux'].to_numpy()\n",
    "        fluxerr = group['fluxerr'].to_numpy()\n",
    "        grid_time = group['grid_time'].to_numpy()\n",
    "        time_index = group['time_index'].to_numpy()\n",
    "\n",
    "\n",
    "        # Retornar un diccionario en el formato deseado\n",
    "        # light_curve_dict = {\n",
    "        #     oid: {\n",
    "        #         'mjd': mjd,\n",
    "        #         'magpsf': magpsf,\n",
    "        #         'fluxerr': fluxerr,\n",
    "        #         'grid_time': grid_time,\n",
    "        #         'time_index': time_index\n",
    "        #     }\n",
    "        # }\n",
    "\n",
    "        light_curve_dict = {\n",
    "          'mjd': torch.tensor(mjd, dtype=torch.float32).to(device),\n",
    "          'flux': torch.tensor(flux, dtype=torch.float32).to(device),\n",
    "          'fluxerr': torch.tensor(fluxerr, dtype=torch.float32).to(device),\n",
    "          'grid_time': torch.tensor(grid_time, dtype=torch.torch.uint32).to(device),\n",
    "          'time_index': torch.tensor(time_index, dtype=torch.torch.uint32).to(device)\n",
    "        }\n",
    "\n",
    "        return light_curve_dict\n",
    "\n",
    "# Instancia el dataset personalizado\n",
    "train_pandas_dataset = LightCurveDataset(train_data_synthetic)\n",
    "test_pandas_dataset = LightCurveDataset(test_data_synthetic)\n",
    "\n",
    "# Crear el DataLoader\n",
    "train_loader = DataLoader(train_pandas_dataset, batch_size=64, collate_fn=list, shuffle=True)\n",
    "test_loader = DataLoader(test_pandas_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    print(type(batch))  # Verifica el tipo de batch\n",
    "    print(batch[0]['time_index'])\n",
    "    print(_get_data(batch))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelV1(nn.Module):\n",
    "    \"\"\" This model will use Conv1d to encode and decode the NN\n",
    "    The Output Size using Conv1d in Pytorch could be calculated using:\n",
    "\n",
    "    output_size = [Lin + 2 x padding - dilation x (kernel_size - 1) - 1] / stride + 1 \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size: int, latent_size: int, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=input_size, out_channels=16, kernel_size=3, dilation=1, padding=2*1),  # Input size [batch_size, 3, input_size]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, dilation=1, padding=2*1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, dilation=1, padding=2*1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Linear layers for encoding mean and logvar\n",
    "        self.encode_mean = nn.Linear(in_features=64, out_features=latent_size)\n",
    "        self.encode_logvar = nn.Linear(in_features=64, out_features=latent_size)\n",
    "\n",
    "        # Decoder input layer\n",
    "        self.decoder_input = nn.Linear(in_features=latent_size, out_features=64)\n",
    "\n",
    "        # Decoder with transposed convolutions (ConvTranspose1d)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_channels=64, out_channels=32, kernel_size=3, padding=2*1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(in_channels=32, out_channels=16, kernel_size=3, padding=2*1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(in_channels=16, out_channels=3, kernel_size=3, padding=2*1)  # Output size [batch_size, 3, input_size]\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        print('Shape Pre-encoder', x.shape)\n",
    "        x = x.permute(0,2,1)\n",
    "        print('second Shape pre-encoder', x.shape)\n",
    "        # Flatten the output from the convolutional layers\n",
    "        #x = x.view(x.size(0), -1)\n",
    "        #print('Flattened shape:', x.shape)\n",
    "        mu, log_var = self.encode_mean(x), self.encode_logvar(x)\n",
    "        print('Shape mu:',mu.shape)\n",
    "        print('Shape log_var:',log_var.shape)\n",
    "        return mu, log_var\n",
    "\n",
    "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        result = self.decoder_input(z)\n",
    "        result = self.decoder(result)\n",
    "        return result\n",
    "\n",
    "    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, input: torch.Tensor, **kwargs) -> list:\n",
    "        print('Shape input:',input.shape)\n",
    "        mu, log_var = self.encode(input)\n",
    "        print('Shape mu:',mu.shape)\n",
    "        print('Shape log_var:',log_var.shape)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return [self.decode(z), input, mu, log_var]\n",
    "\n",
    "    def obtain_results(self, light_curves):\n",
    "\n",
    "        data = _get_data(light_curves)\n",
    "        print('Paso el Get data')\n",
    "\n",
    "        # Encode the light_curves\n",
    "        encoding_mu, encoding_logvar = self.encode(data['input_data'])\n",
    "        #print(encoding_mu)\n",
    "        #print(encoding_logvar)\n",
    "\n",
    "        time = data['compare_data'][:,0]\n",
    "        obs_flux = data['compare_data'][:, 1]\n",
    "        obs_fluxerr = data['compare_data'][:, 2]\n",
    "        obs_weight = data['compare_data'][:, 3]\n",
    "\n",
    "        results ={\n",
    "            'redshift': data['redshift'],\n",
    "            'time': time,\n",
    "            'obs_flux': obs_flux,\n",
    "            'obs_fluxerr': obs_fluxerr,\n",
    "            'obs_weight': obs_weight,\n",
    "            'encoding_mu': encoding_mu,\n",
    "            'encoding_logvar': encoding_logvar,\n",
    "        }\n",
    "\n",
    "        #results = {k:v.detach().cpu().numpy() for k,v in results.items()}\n",
    "        #results = {k:v.detach().cpu().numpy() for k,v in results.items()}\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelV0(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_dim, latent_size, device):\n",
    "\n",
    "        self.device = device\n",
    "        super().__init__()\n",
    "\n",
    "        # encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_features=input_size, out_features=hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_dim, out_features=latent_size),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "\n",
    "        # latent mean and variance\n",
    "        # Acá tenia un dos en\n",
    "        #mu_size, logvar_size = 3, 3\n",
    "        self.encode_mean_layer = nn.Linear(in_features=latent_size, out_features=2)\n",
    "        self.encode_logvar_layer = nn.Linear(in_features=latent_size, out_features=2)\n",
    "\n",
    "        # decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_features=2, out_features=latent_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=latent_size, out_features=hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_dim, out_features=input_size),\n",
    "            )\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "    def encode(self, x):\n",
    "        print('Shape pre-encoder', x.shape)\n",
    "        x = x.permute(0,2,1)\n",
    "        print('second Shape pre-encoder', x.shape)\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        print('Entro al Encoder')\n",
    "        print(x.shape)\n",
    "        mean, logvar = self.encode_mean_layer(x), self.encode_logvar_layer(x)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterization(self, mean, logvar):\n",
    "        print('Esta parametrizando al Encoder')\n",
    "        # Var(x) = std**2 -> 0.5*ln(Var(x)) = ln(std)\n",
    "        # std = exp(0.5*ln(Var(x)))\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mean + eps*std\n",
    "        return z\n",
    "\n",
    "    def decode(self, x):\n",
    "        print('Entro al DeEncoder')\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, logvar = self.encode(x)\n",
    "        z = self.reparameterization(mean, logvar)\n",
    "        x_hat = self.decode(z)\n",
    "        print('Melo el forward')\n",
    "        return x_hat, mean, logvar\n",
    "\n",
    "    def obtain_results(self, light_curves):\n",
    "\n",
    "        data = _get_data(light_curves)\n",
    "        print('Paso el Get data')\n",
    "\n",
    "        # Encode the light_curves\n",
    "        encoding_mu, encoding_logvar = self.encode(data['input_data'])\n",
    "        #print(encoding_mu)\n",
    "        #print(encoding_logvar)\n",
    "\n",
    "        time = data['compare_data'][:,0]\n",
    "        obs_flux = data['compare_data'][:, 1]\n",
    "        obs_fluxerr = data['compare_data'][:, 2]\n",
    "        obs_weight = data['compare_data'][:, 3]\n",
    "\n",
    "        results ={\n",
    "            'redshift': data['redshift'],\n",
    "            'time': time,\n",
    "            'obs_flux': obs_flux,\n",
    "            'obs_fluxerr': obs_fluxerr,\n",
    "            'obs_weight': obs_weight,\n",
    "            'encoding_mu': encoding_mu,\n",
    "            'encoding_logvar': encoding_logvar,\n",
    "        }\n",
    "\n",
    "        #results = {k:v.detach().cpu().numpy() for k,v in results.items()}\n",
    "        #results = {k:v.detach().cpu().numpy() for k,v in results.items()}\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 3 # input_size = 2 * N_bands + 1 -> 2: flux and error 1 for redshift.\n",
    "h_dim = 200\n",
    "z_dim = 20\n",
    "num_epoch = 5\n",
    "batch_size = 32\n",
    "learning_rate = 1e-4\n",
    "\n",
    "model = ModelV0(input_size=input_size, hidden_dim=h_dim, latent_size=z_dim, device=device)\n",
    "#model = ModelV1(input_size = input_size, latent_size = z_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(results):\n",
    "\n",
    "#     print('X_reconstructed:\\nType:',type(X_reconstructed),\\\n",
    "#           '\\nShape:',X_reconstructed.shape)\n",
    "#     print('X_weights:\\nType',type(X_weights),\\\n",
    "#           '\\nShape:',X_weights.shape)\n",
    "      nll = (0.5 * torch.tensor(results['obs_weight']) * torch.tensor(results['obs_flux'])**2)\n",
    "      kld = -0.5 * (1 + torch.tensor(results['encoding_logvar'], requires_grad=True)\n",
    "                      - torch.tensor(results['encoding_mu'], requires_grad=True)**2\n",
    "                      - torch.exp(torch.tensor(results['encoding_logvar'], requires_grad=True)))\n",
    "\n",
    "      nll = torch.sum(nll)\n",
    "      #print(nll)\n",
    "      kl_div = torch.sum(kld)\n",
    "      #print(kl_div)\n",
    "\n",
    "      return nll + kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nan_grads(parameters, value=0.0):\n",
    "    \"\"\"Replace NaN gradients\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    parameters : Iterator[torch.Tensor]\n",
    "        Model parameters, usually you can get them by `model.parameters()`\n",
    "    value : float, optional\n",
    "        Value to replace NaNs with\n",
    "    \"\"\"\n",
    "    for p in parameters:\n",
    "        if p.grad is None:\n",
    "            continue\n",
    "        grads = p.grad.data\n",
    "        grads[torch.isnan(grads)] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "for epoch in tqdm(range(num_epoch)):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    i = 0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        # Mueve los datos del batch al dispositivo (GPU o CPU)\n",
    "        #batch = [{oid: {key: value.to(model.device) for key, value in light_curve.items()}} for light_curve in batch]\n",
    "        print(i)\n",
    "        results = model.obtain_results(batch)\n",
    "        loss = loss_function(results)\n",
    "\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Puedes añadir un mensaje de final de epoch aquí si lo deseas\n",
    "    #if epoch % 10 == 0:\n",
    "    print(f'Epoch [{epoch+1}/{num_epoch}], Loss: {loss.item()/300:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(num_epoch), np.array(train_losses)/300)\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Train loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(time, original_magnitude, reconstructed_magnitude, title='Curvas Originales y Reconstruidas'):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(time, original_magnitude, label='Original', linestyle='--', color='blue')\n",
    "    plt.plot(time, reconstructed_magnitude, label='Reconstruido', linestyle='-', color='red')\n",
    "    plt.xlabel('Tiempo')\n",
    "    plt.ylabel('Magnitud')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Definir una lista para almacenar las pérdidas de cada muestra de prueba\n",
    "test_losses = []\n",
    "\n",
    "# Definir una lista para almacenar las reconstrucciones de las muestras de prueba\n",
    "reconstructions = []\n",
    "\n",
    "#with torch.inference_mode():\n",
    "with torch.inference_mode():\n",
    "    for _, group_oid in test_data_synthetic.groupby(by='oid'):\n",
    "        X = group_oid[['mjd','magpsf','sigmapsf']]\n",
    "        #X = process_light_curve_atat(X)# Karpathy constant is just a joke\n",
    "        X = process_light_curve_parsnip(X)\n",
    "        X, X_weights = create_grid(X)\n",
    "        time = X[0,:]\n",
    "        original_magnitude = X[1,:]\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        X_weights = torch.tensor(X_weights, dtype=torch.float32)\n",
    "        X = X.T\n",
    "        X_weights = X_weights.T\n",
    "\n",
    "        # Forward pass\n",
    "        X_reconstructed, mu, logvar = model(X)\n",
    "\n",
    "        #print(X_reconstructed)\n",
    "        # Convierte las predicciones a numpy\n",
    "        reconstructed_magnitude = X_reconstructed.cpu().numpy()[:,0]  # Segunda fila para magnitud\n",
    "        #print(len(reconstructed_magnitude))\n",
    "        \n",
    "        # Grafica\n",
    "        plot_curves(time, original_magnitude, reconstructed_magnitude, title=f'Curvas para OID {oid}')\n",
    "        \n",
    "        # Sal de la iteración si solo deseas graficar para un OID\n",
    "        break\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(X_reconstructed, X_weights, mu, logvar)\n",
    "        #replace_nan_grads(model.parameters())\n",
    "\n",
    "        # Guardar la pérdida y las reconstrucciones\n",
    "        test_losses.append(loss.item())\n",
    "        reconstructions.append(X_reconstructed.cpu().numpy())\n",
    "\n",
    "    # Calcular la pérdida promedio en los datos de prueba\n",
    "    average_test_loss = np.mean(test_losses)\n",
    "    print(\"Average test loss:\", average_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions = np.array(reconstructions)\n",
    "len(reconstructions[0][:,0]), len(reconstructions[0][:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions[0][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 2\n",
    "sample_data = test_data_synthetic[test_data_synthetic.oid == test_data_synthetic.oid.unique()[sample_index]]\n",
    "sample_data = process_light_curve_parsnip(sample_data)\n",
    "sample_data, _ = create_grid(sample_data)\n",
    "sample_reconstruction = reconstructions[sample_index]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(sample_data[0,:], sample_data[1,:], 'o', color='C0')\n",
    "ax.plot(np.linspace(0,300,300), sample_reconstruction[:,1], 'o', color='C1')\n",
    "\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Flux')\n",
    "ax.set_title('Original vs. Reconstructed Flux')\n",
    "#ax.legend()\n",
    "#ax.invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
